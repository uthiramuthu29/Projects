{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing All Neccesary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv(r\"C:\\Users\\shanm\\Downloads\\heart disease prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Datatypes seems fine. Let's check for Nulls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_age = heart_disease['age'].isnull().sum()\n",
    "total_count_age = len(heart_disease['age'])\n",
    "null_percentage_age = (null_count_age / total_count_age) * 100\n",
    "\n",
    "null_count_slope = heart_disease['slope'].isnull().sum()\n",
    "total_count_slope = len(heart_disease['slope'])\n",
    "null_percentage_slope = (null_count_slope / total_count_slope) * 100\n",
    "\n",
    "print(f\"The percentage of null values in 'age' is: {null_percentage_age:.2f}%\")\n",
    "print(f\"The percentage of null values in 'slope' is: {null_percentage_slope:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very less. So we can drop those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.dropna(inplace= True)\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease['age'] = heart_disease['age'].astype('int64')\n",
    "heart_disease['slope'] = heart_disease['slope'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "for i, col in enumerate(heart_disease.columns):\n",
    "    if col != 'target':\n",
    "        unique_value_count = heart_disease[col].nunique()\n",
    "        if unique_value_count <= 4:\n",
    "            categorical_cols.append(col)\n",
    "        else:\n",
    "            numerical_cols.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = heart_disease[numerical_cols]\n",
    "numerical_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numerical_df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Numerical Columns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(nrows= 2, ncols= 3, figsize= (15,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i , col in enumerate(numerical_cols):\n",
    "    sns.boxplot(x = col, data = heart_disease, ax = axes[i], palette = 'rocket')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Univariate analysis for Continuous Columns\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(nrows= 3, ncols= 3, figsize= (15,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i , col in enumerate(categorical_cols):\n",
    "    sns.countplot(x = col, data = heart_disease, ax = axes[i], palette = 'rocket')\n",
    "\n",
    "for j in range(len(categorical_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Univariate analysis for Categorical Columns\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(nrows= 2, ncols= 3, figsize= (15,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i , col in enumerate(numerical_cols):\n",
    "    sns.histplot(data=heart_disease, x=col, hue='target', kde=True, multiple='stack', palette='rocket', ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Bivariate analysis of Continuous Columns Vs Target Variable\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axes = plt.subplots(nrows= 3, ncols= 3, figsize= (15,10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i , col in enumerate(categorical_cols):\n",
    "    sns.countplot(x = col, hue = 'target', data = heart_disease, ax = axes[i], palette = 'rocket')\n",
    "\n",
    "for j in range(len(categorical_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Bivariate analysis of Categorical Columns Vs Target Variable\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Pie Chart\n",
    "heart_disease['target'].value_counts().plot.pie(autopct='%1.1f%%', ax=axes[0], labels=['No Disease', 'Disease'])\n",
    "axes[0].set_title('Distribution of Heart Disease')\n",
    "\n",
    "# Count Plot\n",
    "sns.countplot(x='target', data=heart_disease, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Heart Disease')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = heart_disease.drop('target', axis = 1)\n",
    "y = heart_disease['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size= 0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_normalized = scaler.fit_transform(x_train)\n",
    "x_test_normalized = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(x_train_normalized, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_normalized)\n",
    "    y_pred_prob = model.predict_proba(x_test_normalized)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "    # Append to results list\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "metrics_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Confusion Matrix for Each Model', fontsize=16)\n",
    "\n",
    "for (name, model), ax in zip(models.items(), axes.flatten()):\n",
    "    # Train the model\n",
    "    model.fit(x_train_normalized, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_normalized)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                annot_kws={'size': 14}, linewidths=0.5, linecolor='black', ax=ax)\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('ROC-AUC Curve for Each Model', fontsize=16)\n",
    "\n",
    "for (name, model), ax in zip(models.items(), axes.flatten()):\n",
    "    # Train the model\n",
    "    model.fit(x_train_normalized, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict_proba(x_test_normalized)[:, 1]\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], '--', color='gray')\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "param_grid_lr = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "grid_search_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_search_lr.fit(x_train_normalized, y_train)\n",
    "\n",
    "# Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 3, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_search_dt.fit(x_train_normalized, y_train)\n",
    "\n",
    "# Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(x_train_normalized, y_train)\n",
    "\n",
    "# SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(x_train_normalized, y_train)\n",
    "\n",
    "# KNN\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_knn.fit(x_train_normalized, y_train)\n",
    "\n",
    "# XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy')\n",
    "grid_search_xgb.fit(x_train_normalized, y_train)\n",
    "\n",
    "# Results\n",
    "results = {\n",
    "    'Logistic Regression': grid_search_lr.best_params_,\n",
    "    'Decision Tree': grid_search_dt.best_params_,\n",
    "    'Random Forest': grid_search_rf.best_params_,\n",
    "    'SVM': grid_search_svm.best_params_,\n",
    "    'KNN': grid_search_knn.best_params_,\n",
    "    'XGBoost': grid_search_xgb.best_params_\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for model, params in results.items():\n",
    "    print(f\"Best Hyperparameters for {model}: {params}\")\n",
    "\n",
    "\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "y_pred_lr = best_lr.predict(x_test_normalized)\n",
    "y_pred_dt = best_dt.predict(x_test_normalized)\n",
    "y_pred_rf = best_rf.predict(x_test_normalized)\n",
    "y_pred_svm = best_rf.predict(x_test_normalized)\n",
    "y_pred_knn = best_rf.predict(x_test_normalized)\n",
    "y_pred_xgb = best_rf.predict(x_test_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression\n",
    "train_score_lr = accuracy_score(y_train, best_lr.predict(x_train_normalized))\n",
    "test_score_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Decision Tree\n",
    "train_score_dt = accuracy_score(y_train, best_dt.predict(x_train_normalized))\n",
    "test_score_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Random Forest\n",
    "train_score_rf = accuracy_score(y_train, best_rf.predict(x_train_normalized))\n",
    "test_score_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# SVM\n",
    "train_score_svm = accuracy_score(y_train, best_svm.predict(x_train_normalized))\n",
    "test_score_svm = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# KNN\n",
    "train_score_knn = accuracy_score(y_train, best_knn.predict(x_train_normalized))\n",
    "test_score_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# XGBoost\n",
    "train_score_xgb = accuracy_score(y_train, best_xgb.predict(x_train_normalized))\n",
    "test_score_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Display scores\n",
    "print(f\"Logistic Regression - Training Score: {train_score_lr:.4f}, Testing Score: {test_score_lr:.4f}\")\n",
    "print(f\"Decision Tree - Training Score: {train_score_dt:.4f}, Testing Score: {test_score_dt:.4f}\")\n",
    "print(f\"Random Forest - Training Score: {train_score_rf:.4f}, Testing Score: {test_score_rf:.4f}\")\n",
    "print(f\"SVM - Training Score: {train_score_svm:.4f}, Testing Score: {test_score_svm:.4f}\")\n",
    "print(f\"KNN - Training Score: {train_score_knn:.4f}, Testing Score: {test_score_knn:.4f}\")\n",
    "print(f\"XGBoost - Training Score: {train_score_xgb:.4f}, Testing Score: {test_score_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the metrics\n",
    "metrics_list = []\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "y_pred_lr_prob = best_lr.predict_proba(x_test_normalized)[:, 1]\n",
    "lr_metrics = {\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'Precision': precision_score(y_test, y_pred_lr),\n",
    "    'Recall': recall_score(y_test, y_pred_lr),\n",
    "    'F1 Score': f1_score(y_test, y_pred_lr),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_lr_prob)\n",
    "}\n",
    "metrics_list.append(lr_metrics)\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "y_pred_dt_prob = best_dt.predict_proba(x_test_normalized)[:, 1]\n",
    "dt_metrics = {\n",
    "    'Model': 'Decision Tree',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
    "    'Precision': precision_score(y_test, y_pred_dt),\n",
    "    'Recall': recall_score(y_test, y_pred_dt),\n",
    "    'F1 Score': f1_score(y_test, y_pred_dt),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_dt_prob)\n",
    "}\n",
    "metrics_list.append(dt_metrics)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "y_pred_rf_prob = best_rf.predict_proba(x_test_normalized)[:, 1]\n",
    "rf_metrics = {\n",
    "    'Model': 'Random Forest',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'Precision': precision_score(y_test, y_pred_rf),\n",
    "    'Recall': recall_score(y_test, y_pred_rf),\n",
    "    'F1 Score': f1_score(y_test, y_pred_rf),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_rf_prob)\n",
    "}\n",
    "metrics_list.append(rf_metrics)\n",
    "\n",
    "# Evaluate Support Vector Machine\n",
    "y_pred_svm_prob = best_svm.predict_proba(x_test_normalized)[:, 1]\n",
    "svm_metrics = {\n",
    "    'Model': 'Support Vector Machine',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_svm),\n",
    "    'Precision': precision_score(y_test, y_pred_svm),\n",
    "    'Recall': recall_score(y_test, y_pred_svm),\n",
    "    'F1 Score': f1_score(y_test, y_pred_svm),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_svm_prob)\n",
    "}\n",
    "metrics_list.append(svm_metrics)\n",
    "\n",
    "# Evaluate K Nearest Neighbours\n",
    "y_pred_knn_prob = best_knn.predict_proba(x_test_normalized)[:, 1]\n",
    "knn_metrics = {\n",
    "    'Model': 'K Nearest Neighbours',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'Precision': precision_score(y_test, y_pred_knn),\n",
    "    'Recall': recall_score(y_test, y_pred_knn),\n",
    "    'F1 Score': f1_score(y_test, y_pred_knn),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_knn_prob)\n",
    "}\n",
    "metrics_list.append(knn_metrics)\n",
    "\n",
    "# Evaluate XG Boost Classifier\n",
    "y_pred_xgb_prob = best_xgb.predict_proba(x_test_normalized)[:, 1]\n",
    "xgb_metrics = {\n",
    "    'Model': 'XG Boost Classifier',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "    'Precision': precision_score(y_test, y_pred_xgb),\n",
    "    'Recall': recall_score(y_test, y_pred_xgb),\n",
    "    'F1 Score': f1_score(y_test, y_pred_xgb),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_xgb_prob)\n",
    "}\n",
    "metrics_list.append(xgb_metrics)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Random Forest appears to be the strongest performer , we chose that as our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = best_rf.predict(x_test_normalized)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', annot_kws={\"size\": 16})\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_prob = best_rf.predict_proba(x_test_normalized)[:, 1]\n",
    "\n",
    "# Compute ROC curve and ROC-AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_rf_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic - Random Forest')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI using Tkinter to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        self.master.title(\"Machine Learning Prediction App\")\n",
    "\n",
    "        # Creating input fields for each feature\n",
    "        self.feature_labels = ['Age',\n",
    "                                'Sex (0 or 1)',\n",
    "                                'CP (0 to 3)',\n",
    "                                'trestbps (90 - 200)',\n",
    "                                'chol (120 - 600)',\n",
    "                                'fbs (0 or 1)',\n",
    "                                'restecg (0 to 2)',\n",
    "                                'thalach (70 - 210)',\n",
    "                                'exang (0 or 1)', \n",
    "                                'oldpeak (0.0 to 10.0)', \n",
    "                                'slope (0 to 2)', \n",
    "                                'ca (0 to 4)', \n",
    "                                'thal (0 to 3)']  \n",
    "        self.feature_entries = []\n",
    "\n",
    "        for i, feature_label in enumerate(self.feature_labels):\n",
    "            label = ttk.Label(master, text=feature_label)\n",
    "            label.grid(row=i, column=0, padx=10, pady=10)\n",
    "\n",
    "            entry = ttk.Entry(master)\n",
    "            entry.grid(row=i, column=1, padx=10, pady=10)\n",
    "            self.feature_entries.append(entry)\n",
    "\n",
    "        # Create buttons for specific actions\n",
    "        predict_button = ttk.Button(master, text=\"Predict\", command=self.predict)\n",
    "        predict_button.grid(row=len(self.feature_labels), column=1, pady=20)\n",
    "\n",
    "        clear_button = ttk.Button(master, text=\"Clear\", command=self.clear_inputs)\n",
    "        clear_button.grid(row=len(self.feature_labels), column=0, pady=20)\n",
    "\n",
    "        # Create a label to display predictions\n",
    "        self.result_label = ttk.Label(master, text=\"\")\n",
    "        self.result_label.grid(row=len(self.feature_labels) + 1, columnspan=2)\n",
    "\n",
    "    def predict(self):\n",
    "        # Get user input values\n",
    "        user_inputs = [float(entry.get()) for entry in self.feature_entries]\n",
    "\n",
    "        # Make predictions using the model\n",
    "        prediction = self.make_prediction(user_inputs)\n",
    "\n",
    "        # Map predicted values to labels\n",
    "        predicted_label = \"Disease\" if prediction == 1 else \"No Disease\"\n",
    "\n",
    "        # Display the prediction\n",
    "        self.result_label.config(text=f\"Predicted Target: {predicted_label}\")\n",
    "\n",
    "    def make_prediction(self, user_inputs):\n",
    "        prediction = best_rf.predict([user_inputs])  \n",
    "        return prediction[0]\n",
    "\n",
    "    def clear_inputs(self):\n",
    "        for entry in self.feature_entries:\n",
    "            entry.delete(0, tk.END)\n",
    "        self.result_label.config(text=\"\")\n",
    "\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    app = PredictionApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
